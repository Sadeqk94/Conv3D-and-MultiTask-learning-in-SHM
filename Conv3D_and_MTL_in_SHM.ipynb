{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Qatar University Grandstand Simulator (QUGS) Damage Detection using Conv3D and MTL\n",
        "\n",
        "In this notebook, we present the implementation of a 3D Convolutional Neural Network (CNN) and multi-task learning approach on the Qatar University Grandstand Simulator (QUGS) dataset to detect structural damages.\n",
        "\n",
        "The QUGS dataset represents a structural system with 30 joints, as illustrated in the image below:\n",
        "![QUGS Joints](http://www.structuralvibration.com/img/i2.png)\n",
        "\n",
        "Each joint is equipped with accelerometers, and damages are induced by loosening the bolts, as depicted in the image below:\n",
        "![Damage Application](http://www.structuralvibration.com/img/i3.png)\n",
        "\n",
        "Our objective in this notebook is to utilize deep learning techniques to detect damages in this structural system using the vibration measurements obtained from the accelerometers. We employ a 3D CNN to capture the spatial and temporal patterns in the data, enabling the accurate identification of structural changes.\n",
        "\n",
        "For a more comprehensive understanding of the structure and the tests conducted, we recommend referring to the following link:\n",
        "[QUGS Benchmark](http://www.structuralvibration.com/benchmark/)\n",
        "\n",
        "Through this notebook, we aim to demonstrate the effectiveness of our proposed approach in detecting structural damages using Deep learning techniques.\n",
        "\n"
      ],
      "metadata": {
        "id": "pvpxb_t1eZAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import"
      ],
      "metadata": {
        "id": "YRPDmf_QqaEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import keras\n",
        "from keras.models import load_model, Model\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "bVJN5L_ARmue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##**************************\n",
        "##****** elavation**********\n",
        "##**************************\n",
        "def plot_performance(hist,epoch, title, save='/content/',Legs=None):\n",
        "    keyss=list(hist.history.keys())\n",
        "    if Legs:\n",
        "      legs=Legs\n",
        "    else:\n",
        "      legs=keyss\n",
        "\n",
        "\n",
        "    colors= ['red','blue','green','#FFA500']\n",
        "\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    for i in range(0,len(keyss),2):\n",
        "      histor = np.array(hist.history[keyss[i+1]])\n",
        "      ax.plot(range(1,epoch+1),histor*100, color=colors[i//2], label=legs[i+1])#\n",
        "\n",
        "    # Add a legend to the plot\n",
        "    # ax.legend(loc='right')\n",
        "\n",
        "    # Set the axis labels and title\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Accuracy (%)')\n",
        "    # ax.set_title('Title')\n",
        "    ax.set_ylim((0,100.5))\n",
        "    ax.set_xlim((1,epoch))\n",
        "    ax.grid(axis='x', color='0.85')\n",
        "    ax.grid(axis='y', color='0.85')\n",
        "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "\n",
        "    ax2 = ax.twinx()\n",
        "\n",
        "    for i in range(0,len(keyss),2):\n",
        "      histor = hist.history[keyss[i]]\n",
        "      ax2.plot(range(1,epoch+1),histor, color=colors[i//2],linestyle='--', label=legs[i])#\n",
        "\n",
        "    # Add a legend to the plot\n",
        "\n",
        "\n",
        "    # Set the axis labels and title\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss',color='red')\n",
        "    ax2.tick_params(axis='y', colors='red')\n",
        "\n",
        "    lines1, labels1 = ax.get_legend_handles_labels()\n",
        "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "    # print(lines1, labels1 ,lines2, labels2 )\n",
        "    ax2.legend(lines1 + lines2, labels1 + labels2, loc='right')\n",
        "    plt.savefig(save+'.png', dpi=800)\n",
        "\n",
        "    # convert the history.history dict to a pandas DataFrame:\n",
        "    hist_df = pd.DataFrame(hist.history)\n",
        "\n",
        "    # or save to csv:\n",
        "    hist_csv_file = save+'-history.csv'\n",
        "    with open(hist_csv_file, mode='w') as f:\n",
        "        hist_df.to_csv(f)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "    if save:\n",
        "      fig.savefig(save+'-'+title+'.jpg',dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#%% Defined Confusion Matrix plot function\n",
        "def PlotConfusionMatrix(y_test ,y_pred_probs, title='Confusion Matrix', figsize=(8,8), save='/content/'):\n",
        "\n",
        "    cf_matrix=confusion_matrix(np.argmax(y_test,axis = 1), np.argmax(y_pred_probs,axis = 1))\n",
        "    # DetaFrame_cm = pdNew.DataFrame(array, range(6), range(5))\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    ax2 = plt.axes()\n",
        "    ax2.set_title(title)\n",
        "    ax2.xaxis.tick_top()\n",
        "    ConfusionMatrixDisplay(cf_matrix).plot(ax=ax2,cmap='Blues',colorbar=False)\n",
        "\n",
        "    if save:\n",
        "      fig.savefig(save+'-'+title+'.jpg', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "def uneqCM(confmatdata,fsize=(10,8),title='Confusion Matrix (Multi Task)',save='/content/'):\n",
        "    fig, ax = plt.subplots(figsize=fsize)\n",
        "    ax=sns.heatmap(confusion_matrix(confmatdata[:,0], confmatdata[:,1])[:-1],cmap='Blues',annot=True,cbar=False)\n",
        "    ax.set(xlabel='Predicted Category',ylabel='True category' )\n",
        "    ax.set_title(title)\n",
        "    ax.xaxis.tick_top()\n",
        "    ax.spines['top'].set_visible(True)\n",
        "    ax.spines['right'].set_visible(True)\n",
        "    ax.spines['left'].set_visible(True)\n",
        "    ax.spines['bottom'].set_visible(True)\n",
        "\n",
        "    if save:\n",
        "      fig.savefig(save+'-'+title+'.jpg', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# claculate model scores\n",
        "def score_model(y_test, y_pred_oh, y_pred_probs, save ='/content/'):\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred_oh)\n",
        "    prec = precision_score(y_test, y_pred_oh, average = 'weighted')\n",
        "    rec = recall_score(y_test, y_pred_oh, average = 'weighted')\n",
        "    fscore = f1_score(np.argmax(y_test,axis = 1), np.argmax(y_pred_oh,axis = 1), average='weighted')\n",
        "    cmatrix = confusion_matrix(np.argmax(y_test,axis = 1), np.argmax(y_pred_oh,axis = 1))\n",
        "    # _, conf = pred_confidence(y_pred_probs)\n",
        "\n",
        "    scores = {'accuracy' : acc,\n",
        "              'precision' : prec,\n",
        "              'recall' : rec,\n",
        "              'F1score': fscore,\n",
        "              # 'Confidence' : conf,\n",
        "              'Confusion': cmatrix\n",
        "              }\n",
        "\n",
        "    if save:\n",
        "        import csv\n",
        "        with open(save+'-Scores','w') as f:\n",
        "            for key in scores.keys():\n",
        "                f.write(\"%s,%s\\n\"%(key,scores[key]))\n",
        "    del scores['Confusion']\n",
        "    return scores\n",
        "\n",
        "# Calculate task-specific accuracy\n",
        "def MT_eval(Y_pred,Y_test,num_tasks,save):\n",
        "  task_acc = []\n",
        "  MT_Mat = np.zeros((num_tasks, 4))\n",
        "  for i in range(num_tasks):\n",
        "      task_y_true = Y_test[:, i]\n",
        "      task_y_pred = Y_pred[:, i]\n",
        "      task_acc.append(accuracy_score(task_y_true, task_y_pred))\n",
        "\n",
        "  # Calculate average accuracy\n",
        "  avg_acc = np.mean(task_acc)\n",
        "\n",
        "\n",
        "  # Plot confusion matrices for all tasks\n",
        "  fig, axes = plt.subplots(nrows=5, ncols=6, figsize=(12, 9))\n",
        "  for i, ax in enumerate(axes.flatten()):\n",
        "      if i < num_tasks:\n",
        "          task_y_true = Y_test[:, i]\n",
        "          task_y_pred = Y_pred[:, i]\n",
        "          MT_Mat[i, 0] = accuracy_score(task_y_true, task_y_pred)\n",
        "          MT_Mat[i, 1] = precision_score(task_y_true, task_y_pred)\n",
        "          MT_Mat[i, 2] = recall_score(task_y_true, task_y_pred)\n",
        "          MT_Mat[i, 3] = f1_score(task_y_true, task_y_pred)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          cm = confusion_matrix(task_y_true, task_y_pred)\n",
        "\n",
        "          if cm.shape == (1, 1):\n",
        "          # add dummy row and column with zero value\n",
        "            cm=np.pad(cm,((0, 1), (0, 1)))\n",
        "\n",
        "          sns.heatmap(cm, annot=True, cmap='Blues', ax=ax, fmt='g',cbar=False)\n",
        "          ax.set_title('Task ' +str(i+1), fontsize=10)\n",
        "          ax.set_ylabel('True', fontsize=10)\n",
        "          ax.set_xlabel('Predicted', fontsize=10)\n",
        "          # ax.set_xticks([0,1])\n",
        "          # ax.set_yticks([0,1])\n",
        "          ax.tick_params(axis='x', labelsize=8)\n",
        "          ax.tick_params(axis='y', labelsize=8)\n",
        "          for text in ax.texts:\n",
        "            text.set_fontsize(8)\n",
        "      else:\n",
        "          fig.delaxes(ax)\n",
        "            # save results to CSV file\n",
        "  np.savetxt(save+'evaluation_metrics.csv', MT_Mat, delimiter=\",\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  fig.savefig(save+'-'+'MT-Eval.png',dpi=500)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "5PsR6xD9Wp4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#loading data and basic model constructor"
      ],
      "metadata": {
        "id": "0LfsqZFI0Jpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "##################### Map3D ###########################\n",
        "#######################################################\n",
        "\n",
        "def Map3D(DataTensor,nrow,TempDimLen,nly,labels=None):\n",
        "  nclass=DataTensor.shape[0]\n",
        "\n",
        "  for idx in range(nclass):\n",
        "      respmat2=DataTensor[idx]\n",
        "      respmat2=np.array(np.vsplit(respmat2, nrow))\n",
        "      Xseti=np.array(np.split(respmat2, DataTensor.shape[2]//TempDimLen, axis=-1))\n",
        "      if not labels:\n",
        "        Yseti=np.zeros((Xseti.shape[0],nclass))\n",
        "        Yseti[:,idx]=1\n",
        "      elif labels:\n",
        "        Yseti=np.zeros((Xseti.shape[0],nly))\n",
        "        if idx!=0:\n",
        "          Yseti[:,[x-1 for x in labels[idx]]]=1\n",
        "\n",
        "      try:\n",
        "        Xset=np.vstack((Xset, Xseti))\n",
        "        Yset=np.vstack((Yset, Yseti))\n",
        "      except:\n",
        "        Xset=Xseti\n",
        "        Yset=Yseti\n",
        "      del(respmat2)\n",
        "  return Xset, Yset\n",
        "\n",
        "\n",
        "#######################################################\n",
        "################### LoadData ##########################\n",
        "#######################################################\n",
        "def DownLoadData(DatasetType='single',mode='cls'):\n",
        "  Dataset=DatasetType.lower()\n",
        "  try:\n",
        "    os.remove('/content/SingleA.npy')\n",
        "    os.remove('/content/SingleB.npy')\n",
        "\n",
        "  except:\n",
        "    pass\n",
        "  url1='https://drive.google.com/uc?export=download&id=1BCxLPz0rAnVK-BUl7fYGMHTlsL-sMZoL'\n",
        "  url2='https://drive.google.com/uc?export=download&id=1-8bSjFdd_2gWrL4rC9XEhityLQlyrrTn'\n",
        "\n",
        "\n",
        "\n",
        "  output1 = '/content/SingleA.npy'\n",
        "  output2 = '/content/SingleB.npy'\n",
        "  gdown.download(url1, output1, quiet=True)\n",
        "  gdown.download(url2, output2, quiet=True)\n",
        "\n",
        "\n",
        "\n",
        "#######################################################\n",
        "################# Dataset constructor #################\n",
        "#######################################################\n",
        "\n",
        "def MakeDataset(nrow,TempDimLen,nly,Dataset='single',mode='cls'):\n",
        "\n",
        "  try:\n",
        "    del(X_test,X_train,Y_test,Y_test)\n",
        "\n",
        "\n",
        "  except:\n",
        "    pass\n",
        "  if not os.path.isfile('/content/SingleB.npy'):\n",
        "    DownLoadData(DatasetType=Dataset)\n",
        "\n",
        "\n",
        "  if Dataset=='single':\n",
        "    SingleA=np.load('SingleA.npy')\n",
        "    Xset,Yset=Map3D(SingleA,nrow,TempDimLen,nly,labels=None)\n",
        "    del(SingleA)\n",
        "    SingleB=np.load('SingleB.npy')\n",
        "    Xset1,Yset1=Map3D(SingleB,nrow,TempDimLen,nly,labels=None)\n",
        "    del(SingleB)\n",
        "\n",
        "    Xset=np.vstack((Xset, Xset1))\n",
        "    del(Xset1)\n",
        "    Yset=np.vstack((Yset, Yset1))\n",
        "    if mode=='mtl':\n",
        "      Yset=Yset[:,1:]\n",
        "\n",
        "  if Dataset=='double':\n",
        "    Double=np.load('Double.npy')\n",
        "    if mode=='mtl':\n",
        "      damagedjoint=[[],[3,26],[7,14],[13,23],[21,25],[23,24]]# List of damaged joints in double damage cases\n",
        "      Xset,Yset=Map3D(Double,nrow,TempDimLen,nly,labels=damagedjoint)\n",
        "    else:\n",
        "      Xset,Yset=Map3D(Double,nrow,TempDimLen,nly,labels=None)\n",
        "\n",
        "  return Xset,Yset"
      ],
      "metadata": {
        "id": "tLaG8agDPu8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model constructor"
      ],
      "metadata": {
        "id": "7w4XWuyQYm1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.set_random_seed(42)\n",
        "def build_conv3D_model(nout=31,fc_dim=32,Mode='cls',Shape=(6,5,512,1)):\n",
        "\n",
        "\n",
        "\n",
        "  inputs = keras.layers.Input(shape=Shape)\n",
        "  x= keras.layers.Conv3D(filters=2, kernel_size=(3,3,51),padding=\"same\", name=\"Conv3D_1\")(inputs)\n",
        "  x= keras.layers.BatchNormalization()(x)\n",
        "  x= keras.layers.ReLU()(x)\n",
        "  x= keras.layers.Conv3D(filters=4, kernel_size=(3,3,21),padding=\"same\", name=\"Conv3D_2\")(x)\n",
        "  x= keras.layers.BatchNormalization()(x)\n",
        "  x= keras.layers.ReLU()(x)\n",
        "  x= keras.layers.Conv3D(filters=8, kernel_size=(3,3,11),padding=\"same\", name=\"Conv3D_3\")(x)\n",
        "  x= keras.layers.BatchNormalization()(x)\n",
        "  x= keras.layers.ReLU()(x)\n",
        "  x= keras.layers.Conv3D(filters=8, kernel_size=(3,3,7),padding=\"same\", name=\"Conv3D_4\")(x)\n",
        "  x= keras.layers.BatchNormalization()(x)\n",
        "  x= keras.layers.ReLU()(x)\n",
        "  x= keras.layers.MaxPooling3D(pool_size=(1, 1, 16), name=\"MaxPooling3D-1\")(x)\n",
        "\n",
        "  x= keras.layers.Conv3D(filters=2, kernel_size=(3,3,11),padding=\"same\", name=\"Conv3D_5\")(x)\n",
        "  x= keras.layers.BatchNormalization()(x)\n",
        "  x= keras.layers.ReLU()(x)\n",
        "  x= keras.layers.Conv3D(filters=4, kernel_size=(3,3,7),padding=\"same\", name=\"Conv3D_6\")(x)\n",
        "  x= keras.layers.BatchNormalization()(x)\n",
        "  x= keras.layers.ReLU()(x)\n",
        "  x= keras.layers.Conv3D(filters=8, kernel_size=(3,3,5),padding=\"same\", name=\"Conv3D_7\")(x)\n",
        "  x= keras.layers.BatchNormalization()(x)\n",
        "  x= keras.layers.ReLU()(x)\n",
        "  x= keras.layers.Conv3D(filters=8, kernel_size=(3,3,3),padding=\"same\", name=\"Conv3D_8\")(x)\n",
        "  x= keras.layers.BatchNormalization()(x)\n",
        "  x= keras.layers.ReLU()(x)\n",
        "  x= keras.layers.MaxPooling3D(pool_size=(2, 2, 8), name=\"MaxPooling3D-2\")(x)\n",
        "\n",
        "\n",
        "\n",
        "  # flatten the features and feed into the Dense network\n",
        "  x = keras.layers.Flatten(name=\"encode_flatten\")(x)\n",
        "\n",
        "\n",
        "  # we arbitrarily used fc_dim units here but feel free to change and see what results you get\n",
        "  x =keras.layers.Dense(fc_dim, name=\"dense_1\")(x)\n",
        "  x= keras.layers.BatchNormalization()(x)\n",
        "  x= keras.layers.ReLU()(x)\n",
        "\n",
        "\n",
        "  x = keras.layers.Dense(fc_dim, name=\"Dense_2\")(x)\n",
        "  x= keras.layers.BatchNormalization()(x)\n",
        "  x= keras.layers.ReLU()(x)\n",
        "\n",
        "  if Mode=='cls':\n",
        "    x=keras.layers.Dense(nout,activation=\"softmax\", name=\"Dense_3\")(x)#For Normal Classification\n",
        "  elif Mode=='mtl':\n",
        "    x=keras.layers.Dense(nout,activation=\"sigmoid\", name=\"Dense_3\")(x)#For Multi-task learing\n",
        "\n",
        "\n",
        "  model=Model(inputs=inputs, outputs=x)\n",
        "  return model"
      ],
      "metadata": {
        "id": "b-S9GmrxTfNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CLS-Single"
      ],
      "metadata": {
        "id": "qGnacw145bTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section classification is done for single damage cases"
      ],
      "metadata": {
        "id": "U93AwoXtZEMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TsetSize=0.2\n",
        "Xset, Yset=MakeDataset(nrow=6,TempDimLen=512,nly=30,Dataset='single',mode='cls')\n",
        "Xset=Xset[...,np.newaxis]\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(Xset, Yset, test_size=TsetSize)\n",
        "del(Xset, Yset)\n",
        "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aekIEQAiPUtr",
        "outputId": "d53b71d8-f3cf-40f8-aec0-98193472b3eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25395, 6, 5, 512), (6349, 6, 5, 512), (25395, 31), (6349, 31))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For `X_train, X_test, Y_train, Y_test` You should get tensor with the following sizes:\n",
        "\n",
        "((25395, 6, 5, 512), (6349, 6, 5, 512), (25395, 31), (6349, 31))"
      ],
      "metadata": {
        "id": "lMY2czOoZakI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nclas=31\n",
        "CLS_SingleModel = build_conv3D_model(nout=31,fc_dim=8,Mode='cls')\n",
        "CLS_SingleModel.summary()\n"
      ],
      "metadata": {
        "id": "ZNsUi8KYl1Rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lrr=1e-3\n",
        "EPOCHS = 30\n",
        "\n",
        "step = tf.Variable(0, trainable=False)\n",
        "boundaries = [10,20]\n",
        "values = [lrr,lrr/10,lrr/100]\n",
        "learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "boundaries, values)\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate =learning_rate_fn(step))#\n",
        "\n",
        "\n",
        "CLS_SingleModel.compile(loss=keras.losses.CategoricalCrossentropy(),optimizer=optimizer,metrics='accuracy')# metrics='accuracy')keras.losses.SparseCategoricalCrossentropy()\n",
        "# Store training stats\n",
        "# Save model at each epoch\n",
        "if not os.path.exists(\"Models\"):\n",
        "    os.mkdir(\"Models\")\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath = 'Models/BestModel_cls_single.h5',\n",
        "    monitor='val_loss',#'val_accuracy',\n",
        "    mode='min',\n",
        "    save_best_only=True,\n",
        "    save_weights_only = False)\n",
        "\n",
        "history = CLS_SingleModel.fit(X_train, Y_train, epochs=EPOCHS,batch_size=128, validation_data=( X_test, Y_test),\n",
        "                     verbose=1,callbacks = [model_checkpoint_callback])#"
      ],
      "metadata": {
        "id": "4aue2ZtPPnC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_class=nclas\n",
        "bestmodel = load_model('Models/BestModel_cls_single.h5')\n",
        "y_pred_probs = bestmodel.predict(X_test)   # Softmax class probabilities from model\n",
        "y_pred = np.argmax(y_pred_probs, axis = 1)\n",
        "y_pred_oh = to_categorical(y_pred, n_class)\n",
        "\n",
        "plot_performance(history,EPOCHS,'Learning Curve',save='/content/cls_single')\n",
        "PlotConfusionMatrix( Y_test, y_pred_probs,figsize=(20,20),save='/content/cls_single')\n",
        "\n",
        "# model scores\n",
        "scores=score_model(Y_test, y_pred_oh, y_pred_probs,save = '/content/cls_single.csv')\n",
        "# pred_confidence(y_pred_probs)\n",
        "pd.DataFrame(scores,index=[0])"
      ],
      "metadata": {
        "id": "OMlP4We1ZwnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Double Damage Cases Dataset Availability and Usage Instructions\n",
        "\n",
        "The double damage cases dataset used in this experiment is not publicly available. We obtained access to this dataset through the QUGS lab, and we are grateful for their cooperation. Unfortunately, we are unable to provide the dataset here due to restrictions.\n",
        "\n",
        "If you intend to reproduce the experiments related to the double damage cases, you would need to obtain the data yourself. To do this, you can request access to the double damage cases dataset from the individuals mentioned in http://www.structuralvibration.com/. If they grant access, they will provide you with a text file containing the data, not a Numpy array.\n",
        "\n",
        "To integrate the dataset into the following experiments, please follow these steps:\n",
        "1. Download the double damage cases text files\n",
        "2. Download the one undamaged text file from http://www.structuralvibration.com/benchmark/download/.\n",
        "3. Place these downloaded files into a directory.\n",
        "4. Rename the files as 'zzzDD0.TXT', 'zzzDD1.TXT', ..., 'zzzDD5.TXT'.\n",
        "\n",
        "'zzzDD0.TXT' refers to undamage case\n",
        "\n",
        "After preparing the data as mentioned above, you can utilize the provided code snippets to preprocess the dataset for your experiments. The code will load the text files, process them, and concatenate them into a format suitable for further analysis.\n",
        "\n",
        "```python\n",
        "# Replace 'directory' with the actual path to the directory containing the text files.\n",
        "directory = '/text_files_directory/'\n",
        "\n",
        "for i in range(0, 6):\n",
        "    respmat = np.loadtxt(directory + 'zzzDD' + str(i) + '.TXT', skiprows=11)\n",
        "    respmat2 = np.delete(respmat, 0, 1).T[np.newaxis, ...]\n",
        "\n",
        "    try:\n",
        "        Double = np.concatenate((Double, respmat2), axis=0)\n",
        "        del(respmat, respmat2)\n",
        "    except:\n",
        "        Double = respmat2\n",
        "        del(respmat, respmat2)\n",
        "\n",
        "print('Dataset shape: ', Double.shape)\n",
        "np.save('Double.npy', Double)\n",
        "```\n",
        "\n",
        "Please ensure that the dataset shape matches the expected shape of (6, 30, 262144), and that the file 'Double.npy' is saved in the current directory.\n",
        "\n",
        "Once the dataset is prepared using the provided code, you can proceed to run the subsequent sections of the experiment.\n"
      ],
      "metadata": {
        "id": "-fAo38TuMOz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CLS-Double"
      ],
      "metadata": {
        "id": "0VG6PxNcaf7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  del(X_train, X_test, Y_train, Y_test)\n",
        "except:\n",
        "  pass\n",
        "TsetSize=0.2\n",
        "Xset, Yset=MakeDataset(nrow=6,TempDimLen=512,nly=30,Dataset='double',mode='cls')\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(Xset, Yset, test_size=TsetSize)\n",
        "del(Xset, Yset)\n",
        "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
      ],
      "metadata": {
        "id": "WSQZSq7smoL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For `X_train, X_test, Y_train, Y_test` You should get tensor with the following sizes:\n",
        "\n",
        "((2457, 6, 5, 512), (615, 6, 5, 512), (2457, 6), (615, 6))"
      ],
      "metadata": {
        "id": "1Zy5vv9Ea5AE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nclas=6 # 5 double damage cases plus one undamaged case\n",
        "CLS_DoubleModel = build_conv3D_model(nout=nclas,fc_dim=8,Mode='cls')\n",
        "CLS_DoubleModel.summary()\n"
      ],
      "metadata": {
        "id": "HLkSQeufU-yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lrr=1e-3\n",
        "EPOCHS = 30\n",
        "\n",
        "step = tf.Variable(0, trainable=False)\n",
        "boundaries = [10,20]\n",
        "values = [lrr,lrr/10,lrr/100]\n",
        "learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "boundaries, values)\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate =learning_rate_fn(step))\n",
        "\n",
        "\n",
        "CLS_DoubleModel.compile(loss=keras.losses.CategoricalCrossentropy(),optimizer=optimizer,metrics='accuracy')#\n",
        "# Store training stats\n",
        "# Save model at each epoch\n",
        "if not os.path.exists(\"Models\"):\n",
        "    os.mkdir(\"Models\")\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath = 'Models/BestModel_cls_double.h5',\n",
        "    monitor='val_accuracy',#'val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    save_weights_only = False)\n",
        "\n",
        "history = CLS_DoubleModel.fit(X_train, Y_train, epochs=EPOCHS,batch_size=128, validation_data=( X_test, Y_test),\n",
        "                     verbose=1,callbacks = [model_checkpoint_callback])#"
      ],
      "metadata": {
        "id": "strV7-w2a90a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_class=nclas\n",
        "bestmodel = load_model('Models/BestModel_cls_double.h5')\n",
        "y_pred_probs = bestmodel.predict(X_test)   # Softmax class probabilities from model\n",
        "y_pred = np.argmax(y_pred_probs, axis = 1)\n",
        "y_pred_oh = to_categorical(y_pred, n_class)\n",
        "\n",
        "plot_performance(history,EPOCHS,'Learning Curve',save='/content/cls_double')\n",
        "PlotConfusionMatrix( Y_test, y_pred_probs,figsize=(8,8),save='/content/cls_double')\n",
        "\n",
        "# model scores\n",
        "scores=score_model(Y_test, y_pred_oh, y_pred_probs,save = '/content/cls_double.csv')\n",
        "# pred_confidence(y_pred_probs)\n",
        "pd.DataFrame(scores,index=[0])"
      ],
      "metadata": {
        "id": "hX1BTCcnUlxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CLS-Double-TL\n"
      ],
      "metadata": {
        "id": "pxTZcyHkbQIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  del(X_train, X_test, Y_train, Y_test)\n",
        "except:\n",
        "  pass\n",
        "TsetSize=0.2\n",
        "Xset, Yset=MakeDataset(nrow=6,TempDimLen=512,nly=30,Dataset='double',mode='cls')\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(Xset, Yset, test_size=TsetSize)\n",
        "del(Xset, Yset)\n",
        "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
      ],
      "metadata": {
        "id": "nyZdTYmqvwuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For `X_train, X_test, Y_train, Y_test` You should get tensor with the following sizes:\n",
        "\n",
        "((2457, 6, 5, 512), (615, 6, 5, 512), (2457, 6), (615, 6))"
      ],
      "metadata": {
        "id": "9lOrBbAXbCpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Note The besed save model should be provided in the for below line in the specified directory\n",
        "BestModel_cls_single = load_model('Models/BestModel_cls_single.h5')#.layers[:-6].output\n",
        "xx = keras.layers.Dense(6,activation='softmax')(BestModel_cls_single.layers[-2].output)\n",
        "CLS_Double_TL_Model=Model(inputs=BestModel_cls_single.input,outputs=xx)\n",
        "\n",
        "for layer in CLS_Double_TL_Model.layers[:-7]:\n",
        "  layer.trainable = False\n",
        "\n",
        "CLS_Double_TL_Model.summary()"
      ],
      "metadata": {
        "id": "VVSARIS8bPGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lrr=1e-3\n",
        "EPOCHS = 30\n",
        "\n",
        "step = tf.Variable(0, trainable=False)\n",
        "boundaries = [10,20]\n",
        "values = [lrr,lrr/10,lrr/100]\n",
        "learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "boundaries, values)\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate =learning_rate_fn(step))\n",
        "\n",
        "\n",
        "CLS_Double_TL_Model.compile(loss=keras.losses.CategoricalCrossentropy(),optimizer=optimizer,metrics='accuracy')# metrics='accuracy')keras.losses.SparseCategoricalCrossentropy()\n",
        "# Store training stats\n",
        "# Save model at each epoch\n",
        "if not os.path.exists(\"Models\"):\n",
        "    os.mkdir(\"Models\")\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath = 'Models/BestModel_cls_double(TL).h5',\n",
        "    monitor='val_accuracy',#'val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    save_weights_only = False)\n",
        "\n",
        "history = CLS_Double_TL_Model.fit(X_train, Y_train, epochs=EPOCHS,batch_size=128, validation_data=( X_test, Y_test),\n",
        "                     verbose=1,callbacks = [model_checkpoint_callback])#"
      ],
      "metadata": {
        "id": "vbu5DuP-a-Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_class=6\n",
        "bestmodel = load_model('Models/BestModel_cls_double(TL).h5')\n",
        "y_pred_probs = bestmodel.predict(X_test)   # Softmax class probabilities from model\n",
        "y_pred = np.argmax(y_pred_probs, axis = 1)\n",
        "y_pred_oh = to_categorical(y_pred, n_class)\n",
        "\n",
        "plot_performance(history,EPOCHS,'Learning Curve',save='/content/cls_double(TL)')\n",
        "PlotConfusionMatrix( Y_test, y_pred_probs,figsize=(8,8),save='/content/cls_double(TL)')\n",
        "\n",
        "# model scores\n",
        "scores=score_model(Y_test, y_pred_oh, y_pred_probs,save = '/content/cls_double(TL).csv')\n",
        "# pred_confidence(y_pred_probs)\n",
        "pd.DataFrame(scores,index=[0])"
      ],
      "metadata": {
        "id": "mXtn19IxcwLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multi-Task Learning"
      ],
      "metadata": {
        "id": "XEOWz_0v5MQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  del(X_train, X_test, Y_train, Y_test)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "TsetSize=0.2\n",
        "Xset, Yset=MakeDataset(nrow=6,TempDimLen=512,nly=30,Dataset='single',mode='mtl')\n",
        "Xset1, Yset1=MakeDataset(nrow=6,TempDimLen=512,nly=30,Dataset='double',mode='mtl')\n",
        "Xset1, Yset1= Xset1[512:], Yset1[512:]\n",
        "Xset=np.vstack((Xset,Xset1))\n",
        "Yset=np.vstack((Yset,Yset1))\n",
        "del(Xset1, Yset1)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(Xset, Yset, test_size=TsetSize)\n",
        "del(Xset, Yset)\n",
        "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
      ],
      "metadata": {
        "id": "Gipb7c2Bp87s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For `X_train, X_test, Y_train, Y_test` You should get tensor with the following sizes:\n",
        "\n",
        "((27443, 6, 5, 512), (6861, 6, 5, 512), (27443, 30), (6861, 30))"
      ],
      "metadata": {
        "id": "1oNrQbkhbXzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of 1's in each sample of Yset\n",
        "n_ones = np.sum(Y_test, axis=1)\n",
        "\n",
        "# Separate samples with one 1 and two 1's\n",
        "one_1_mask = (n_ones == 1)\n",
        "two_1_mask = (n_ones == 2)\n"
      ],
      "metadata": {
        "id": "vFyMvW4n5Uii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ntask=30\n",
        "MTL_Model = build_conv3D_model(nout=ntask,fc_dim=8,Mode='mtl')\n",
        "MTL_Model.summary()"
      ],
      "metadata": {
        "id": "3WDiVlfh5VWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This class Allows us to evaluate the model for multiple test sets\n",
        "\n",
        "class AdditionalValidationSets(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, validation_sets1,validation_sets2, verbose=0, batch_size=None):\n",
        "        \"\"\"\n",
        "        :param validation_sets:\n",
        "        a list of 3-tuples (validation_data, validation_targets, validation_set_name)\n",
        "        or 4-tuples (validation_data, validation_targets, sample_weights, validation_set_name)\n",
        "        :param verbose:\n",
        "        verbosity mode, 1 or 0\n",
        "        :param batch_size:\n",
        "        batch size to be used when evaluating on the additional datasets\n",
        "        \"\"\"\n",
        "        super(AdditionalValidationSets, self).__init__()\n",
        "        self.validation_sets1 = validation_sets1\n",
        "        for validation_set in self.validation_sets1:\n",
        "            if len(validation_set) not in [3, 4]:\n",
        "                raise ValueError()\n",
        "\n",
        "        self.validation_sets2 = validation_sets2\n",
        "        for validation_set in self.validation_sets2:\n",
        "            if len(validation_set) not in [3, 4]:\n",
        "                raise ValueError()\n",
        "\n",
        "        self.epoch = []\n",
        "        self.history = {}\n",
        "        self.verbose = verbose\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.epoch = []\n",
        "        self.history = {}\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        self.epoch.append(epoch)\n",
        "\n",
        "        # record the same values as History() as well\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "        # evaluate on the additional validation sets\n",
        "        for validation_set in self.validation_sets1:\n",
        "            if len(validation_set) == 3:\n",
        "                validation_data, validation_targets, validation_set_name = validation_set\n",
        "                sample_weights = None\n",
        "            elif len(validation_set) == 4:\n",
        "                validation_data, validation_targets, sample_weights, validation_set_name = validation_set\n",
        "            else:\n",
        "                raise ValueError()\n",
        "\n",
        "            results = self.model.evaluate(x=validation_data,\n",
        "                                          y=validation_targets,\n",
        "                                          verbose=self.verbose,\n",
        "                                          sample_weight=sample_weights,\n",
        "                                          batch_size=self.batch_size)\n",
        "\n",
        "            for metric, result in zip(self.model.metrics_names,results):\n",
        "                valuename = validation_set_name + '_' + metric\n",
        "                self.history.setdefault(valuename, []).append(result)\n",
        "\n",
        "        for validation_set in self.validation_sets2:\n",
        "            if len(validation_set) == 3:\n",
        "                validation_data, validation_targets, validation_set_name = validation_set\n",
        "                sample_weights = None\n",
        "            elif len(validation_set) == 4:\n",
        "                validation_data, validation_targets, sample_weights, validation_set_name = validation_set\n",
        "            else:\n",
        "                raise ValueError()\n",
        "\n",
        "            results = self.model.evaluate(x=validation_data,\n",
        "                                          y=validation_targets,\n",
        "                                          verbose=self.verbose,\n",
        "                                          sample_weight=sample_weights,\n",
        "                                          batch_size=self.batch_size)\n",
        "\n",
        "            for metric, result in zip(self.model.metrics_names,results):\n",
        "                valuename = validation_set_name + '_' + metric\n",
        "                self.history.setdefault(valuename, []).append(result)"
      ],
      "metadata": {
        "id": "eDIvMaMz6JZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lrr=1e-3\n",
        "EPOCHS = 10\n",
        "\n",
        "step = tf.Variable(0, trainable=False)\n",
        "boundaries = [10,20]\n",
        "values = [lrr,lrr/10,lrr/100]\n",
        "learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "boundaries, values)\n",
        "\n",
        "def MT_accuracy(y_true, y_pred):\n",
        "    y_true = tf.cast(tf.round(y_true), dtype=tf.float32)\n",
        "    y_pred = tf.cast(tf.round(y_pred), dtype=tf.float32)\n",
        "    ns=tf.shape(y_true)[0]\n",
        "    count=0\n",
        "    for i in range(ns):\n",
        "      if tf.experimental.numpy.array_equal( y_true[i,:], y_pred[i,:]):\n",
        "        count+=1\n",
        "    return count/ns\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate =learning_rate_fn(step))\n",
        "\n",
        "MTL_Model.compile(loss=keras.losses.BinaryCrossentropy(),optimizer=optimizer,metrics=[MT_accuracy])# metrics='accuracy')keras.losses.SparseCategoricalCrossentropy()\n",
        "# Store training stats\n",
        "# Save model at each epoch\n",
        "if not os.path.exists(\"Models\"):\n",
        "    os.mkdir(\"Models\")\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath = 'Models/BestMTL_Model.h5',\n",
        "    monitor='val_MT_accuracy',#'val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    save_weights_only = False)\n",
        "\n",
        "HIST = AdditionalValidationSets([(X_test[one_1_mask],Y_test[one_1_mask], 'val: Single Damage')], [(X_test[two_1_mask],Y_test[two_1_mask], 'val: Double Damage')])\n",
        "\n",
        "MTL_Model.fit(X_train, Y_train, epochs=EPOCHS,batch_size=64, validation_data=( X_test, Y_test),\n",
        "                     verbose=1,callbacks = [model_checkpoint_callback,HIST])#"
      ],
      "metadata": {
        "id": "RXE5Vu995IAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bestmodel = load_model('Models/BestMTL_Model.h5',custom_objects={\"MT_accuracy\": MT_accuracy})\n",
        "y_pred_probs = bestmodel.predict(X_test)   # Softmax class probabilities from model\n",
        "plot_performance(HIST,EPOCHS,'Learning Curve',save='/content/MTL')"
      ],
      "metadata": {
        "id": "rVPECCXBKOGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.round(y_pred_probs).astype(int)\n",
        "# Calculate task-specific accuracy\n",
        "MT_eval(y_pred,Y_test,30,save='/content/MTL')\n"
      ],
      "metadata": {
        "id": "5wtLj6Y55kQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-z5A7c1vfHPJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
